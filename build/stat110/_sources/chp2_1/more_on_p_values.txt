More on Using p-Values to Make Decisions
----------------------------------------

As mentioned earlier, some researchers advocate using the following
guidelines:

.. admonition:: p-Value Guidelines

    -  If the p-value falls below .05, there is strong evidence to support the research hypothesis.
    -  If the p-value falls below .10 but above .05, there is “marginal” evidence to support the research hypothesis.
    -  If the p-value is above .10, there is not enough evidence to support the research hypothesis.

In general, the smaller the p-value, the less likely the results of
the study are due to random chance; thus, the more evidence we have
that to support the research hypothesis. In some disciplines, the
p-value must be much smaller than .05 in order to support a research
hypothesis. For example, physics journals often like to see p < .001.
Though the above guidelines allow for claims of “marginal” evidence
when p-values fall between .05 and .10, some statisticians caution
against this. For example, Irwin Bross argues that such modifications
would be detrimental in evaluating evidence.

    *Anyone familiar with certain areas of the scientific literature
    will be well aware of the need for curtailing language-games. Thus
    if there were no 5% level firmly established, then some persons
    would stretch the level to 6% or 7% to prove their point. Soon
    others would be stretching to 10% and 15% and the jargon would
    become meaningless. Whereas nowadays a phrase such as statistically
    significant difference provides some assurance that the results are
    not merely a manifestation of sampling variation, the phrase would
    mean very little if everyone played language-games. To be sure,
    there are always a few folks who fiddle with significance
    levels--who will switch from two-tailed to one-tailed tests or from
    one significance test to another in an effort to get positive
    results. However, such gamesmanship is severely frowned upon.*

    .. admonition:: Source:  
    
        Bross IDJ (1971), "Critical Levels, Statistical Language and Scientific Inference," in Foundations of Statistical Inference.

The “.05 rule” is usually attributed to R.A. Fisher. His published
thoughts on the matter are given below.

    *In the investigation of living beings by biological methods
    statistical tests of significance are essential. Their function is
    to prevent us being deceived by accidental occurrences, due not to
    the causes we wish to study, or are trying to detect, but to a
    combination of the many other circumstances which we cannot control.
    An observation is judged significant, if it would rarely have been
    produced, in the absence of a real cause of the kind we are seeking.
    It is a common practice to judge a result significant, if it is of
    such a magnitude that it would have been produced by chance not more
    frequently than once in twenty trials. This is an arbitrary, but
    convenient, level of significance for the practical investigator,
    but it does not mean that he allows himself to be deceived once in
    every twenty experiments. The test of significance only tells him
    what to ignore, namely all experiments in which significant results
    are not obtained. He should only claim that a phenomenon is
    experimentally demonstrable when he knows how to design an
    experiment so that it will rarely fail to give a significant result.
    Consequently, isolated significant results which he does not know
    how to reproduce are left in suspense pending further
    investigation.*

    .. admonition:: Source:  
    
        R.A. Fisher (1929), “The Statistical Method in Psychical Research,” from the* Proceedings of the Society for Psychical Research\ *, 39, 189-191.  *
